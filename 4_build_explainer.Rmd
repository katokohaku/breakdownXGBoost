---
title: "XGBoostExplainerが何をやっているか調べる"
author: Satoshi Kato (@katokohaku)
output: 
  html_document:
    keep_md: yes
    toc: yes
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)

knitr::opts_knit$set(
  progress = TRUE, 
  verbose = TRUE, 
  root.dir = "."
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

# 目的

今回は、`xgboostExplainer`によって、xgboostの学習済みモデルから**ルールがどうやって抽出されているか**にフォーカスして追いかける。

# 関連シリーズ

1. とりあえず使ってみる
2. 予測結果の可視化プロセスをstep-by-stepで実行する
3. 学習したxgboostのルール抽出をstep-by-stepで実行する（この記事）
4. 予測結果のbreakdownをstep-by-stepで実行する


# 準備：XGBモデルの学習と予測

`xgboostExplainer`のマニュアルにあるexampleからコピペ。

```{r train.and.predict, message=FALSE, results="hide"}
require(tidyverse)
library(xgboost)
library(xgboostExplainer)

set.seed(123)

data(agaricus.train, package='xgboost')

X = as.matrix(agaricus.train$data)
y = agaricus.train$label
table(y)
train_idx = 1:5000

train.data = X[train_idx,]
test.data = X[-train_idx,]

xgb.train.data <- xgb.DMatrix(train.data, label = y[train_idx])
xgb.test.data <- xgb.DMatrix(test.data)

param <- list(objective = "binary:logistic")
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=3)

```


# 学習したxgboostのルール抽出


`buildExplainer()`の中身を抜き書きしながら、step-by-stepで眺める


```{r}
# explainer = buildExplainer(xgb.model,xgb.train.data, type="binary", base_score = 0.5, trees = NULL)
# function (xgb.model, trainingData, type = "binary", base_score = 0.5, trees_idx = NULL) 
# {
trainingData = xgb.train.data
type = "binary" 
base_score = 0.5
trees_idx = NULL

```

## モデルからパスを抽出

[`xgboost::xgb.model.dt.tree()`](https://www.imsbio.co.jp/RGM/R_rdfile?f=xgboost/man/xgb.model.dt.tree.Rd&d=R_CC)を使う。Qualityはそのルールでのsplitによる不純度の改善力、Coverはそのルールの登場頻度くらいに思っておけばよい（投げ遣り）。

```{r}
col_names = attr(trainingData, ".Dimnames")[[2]]
col_names %>% head()

cat("\nCreating the trees of the xgboost model...")
trees = xgb.model.dt.tree(col_names, model = xgb.model, trees = trees_idx)

trees %>% 
  mutate(Feature = str_trunc(Feature, width=12, side="left")) %>%
  mutate_at(.vars = vars("Quality","Cover"), .funs = round)

```
leafの内訳はこんな感じ。

```{r}
trees %>% filter(Feature == "Leaf")%>% select(Tree:Feature)
```


[`predict(..., predleaf = TRUE)`](https://www.rdocumentation.org/packages/xgboost/versions/0.71.2/topics/predict.xgb.Booster)を指定することで、予測値の代わりに訓練データのインスタンスが各treeで所属するLeafのノード番号を取得できる。
今回は`NROW(trainingData)=5000, nrounds = 3`なので、5000行3列の所属Leafの行列が得られる。

```{r}
cat("\nGetting the leaf nodes for the training set observations...")
nodes.train = predict(xgb.model, trainingData, predleaf = TRUE)

nodes.train %>% dim()
nodes.train <- NULL
```

ただし、取得されたのち、学習に使われたインスタンスの予測結果の情報が、`buildExplainer()`のどこかで使われている形跡は見当たらなかった((この実装、全般的にレガシーが残っている傾向があって、追いかけていったら途中で行き止まりだった、みたいなことが、あちこちである))。

## 各roundの予測ルールを抽出・整理する

`buildExplainer()`のエンジン部分。`xgboostExplainer:::getStatsForTrees()`により処理される。

```{r, eval=FALSE}
# cat("\nBuilding the Explainer...")
# cat("\n\nSTEP 2 of 2")
tree_list = xgboostExplainer:::getStatsForTrees(trees, nodes.train, type = type, base_score = base_score)

```

以下では、`xgboostExplainer:::getStatsForTrees()`の一連のステップをトレースする。関数内部で`data.table::copy()'による`- attr(*, ".internal.selfref")=<externalptr>`によって、**データの更新が参照渡しとして行われている**のに注意((これがまたトレーサビリティを下げている))。

### Cover(H)の再計算

`xgb.model.dt.tree()`の情報を根拠に予測値を算出するためのパスと予測値の更新量を取り出す。`xgb.model.dt.tree()`のCoverは精度に難がある((途中の丸め方に問題があるのかな))とのことで、予測に必要になるのでちゃんとした制度で計算しなおし。

```{r}
tree_datatable = data.table::copy(trees)
tree_datatable[, `:=`(leaf, Feature == "Leaf")]
tree_datatable[, `:=`(H, Cover)]
non.leaves = which(tree_datatable[, leaf] == F)

cat("\n\nRecalculating the cover for each non-leaf... \n")

for (i in rev(non.leaves)) {
  left = tree_datatable[i, Yes]
  right = tree_datatable[i, No]
  tree_datatable[i, `:=`(H, tree_datatable[ID == left, H] + tree_datatable[ID == right, H])]
  print(tree_datatable[i,])
}

base_weight = log(base_score/(1 - base_score))
  
tree_datatable[leaf==T,weight:=base_weight + Quality]

tree_datatable[,previous_weight:=base_weight]
tree_datatable[1,previous_weight:=0]

tree_datatable[leaf==T,G:=-weight*H]
tree_datatable %>% str()

tree_list = split(tree_datatable,as.factor(tree_datatable$Tree))
tree_list %>% str()
```

```{r}
num_tree_list = length(tree_list)
treenums =  as.character(0:(num_tree_list-1))
t = 0
cat('\n\nFinding the stats for the xgboost trees...\n')
# pb <- txtProgressBar(style=3)
for (tree in tree_list){
  t=t+1
  num_nodes = nrow(tree)
  non_leaf_rows = rev(which(tree[,leaf]==F))
  for (r in non_leaf_rows){
    left = tree[r,Yes]
    right = tree[r,No]
    leftG = tree[ID==left,G]
    rightG = tree[ID==right,G]
    
    tree[r,G:=leftG+rightG]
    w=tree[r,-G/H]
    
    tree[r,weight:=w]
    tree[ID==left,previous_weight:=w]
    tree[ID==right,previous_weight:=w]
  }
  
  tree[,uplift_weight:=weight-previous_weight]
  # setTxtProgressBar(pb, t / num_tree_list)
}

tree_list

```
```{r, eval=FALSE}
cat("\n\nSTEP 2 of 2")
explainer = xgboostExplainer:::buildExplainerFromTreeList(tree_list, col_names)

```
```{r, eval=TRUE}
# explainer = xgboostExplainer:::buildExplainerFromTreeList(tree_list, col_names)
# function (tree_list, col_names) 
# {
  
tree_list_breakdown <- vector("list", length(col_names) + 3)
names(tree_list_breakdown) = c(col_names, "intercept", "leaf", "tree")
num_trees = length(tree_list)

cat("\n\nGetting breakdown for each leaf of each tree...\n")

for (x in 1:num_trees) {
  tree = tree_list[[x]]
  tree_breakdown = xgboostExplainer:::getTreeBreakdown(tree, col_names)
  tree_breakdown$tree = x - 1
  
  tree_list_breakdown = 
    data.table::rbindlist(append(list(tree_list_breakdown), list(tree_breakdown)))
  
}

explainer = tree_list_breakdown
# return(tree_list_breakdown)
# }

```

```{r}

showWaterfall(xgb.model, explainer, xgb.test.data, test.data,  2, type = "binary")

```

