---
title: "XGBoostExplainerが何をやっているか調べる"
author: Satoshi Kato (@katokohaku)
output: 
  html_document:
    keep_md: yes
    toc: yes
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)

knitr::opts_knit$set(
  progress = TRUE, 
  verbose = TRUE, 
  root.dir = "."
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

# 目的

今回は、`xgboostExplainer`によって、xgboostの学習済みモデルから**ルールがどうやって抽出されているか**にフォーカスし、適宜xgboostの資料を見ながら追いかける。

Introduction to Boosted Trees (PDF)
https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf

XGBoost: A Scalable Tree Boosting System
https://arxiv.org/abs/1603.02754

# 関連シリーズ

1. とりあえず使ってみる
2. 予測結果の可視化プロセスをstep-by-stepで実行する
3. 学習したxgboostのルール抽出をstep-by-stepで実行する（この記事）
4. 予測結果のbreakdownをstep-by-stepで実行する


# 準備：XGBモデルの学習と予測

`xgboostExplainer`のマニュアルにあるexampleからコピペ。

```{r train.and.predict, message=FALSE, results="hide"}
require(tidyverse)
library(xgboost)
library(xgboostExplainer)

set.seed(123)

data(agaricus.train, package='xgboost')

X = as.matrix(agaricus.train$data)
y = agaricus.train$label
table(y)
train_idx = 1:5000

train.data = X[train_idx,]
test.data = X[-train_idx,]

xgb.train.data <- xgb.DMatrix(train.data, label = y[train_idx])
xgb.test.data <- xgb.DMatrix(test.data)

param <- list(objective = "binary:logistic")
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=3)

```


# 学習したxgboostのルール抽出


`buildExplainer()`の中身を抜き書きしながら、step-by-stepで眺める


```{r}
# explainer = buildExplainer(xgb.model,xgb.train.data, type="binary", base_score = 0.5, trees = NULL)
# function (xgb.model, trainingData, type = "binary", base_score = 0.5, trees_idx = NULL) 
# {
trainingData = xgb.train.data
type = "binary" 
base_score = 0.5
trees_idx = NULL

```

## xgb.model.dt.tree()によるパスを抽出

[`xgboost::xgb.model.dt.tree()`](https://www.imsbio.co.jp/RGM/R_rdfile?f=xgboost/man/xgb.model.dt.tree.Rd&d=R_CC)を使う。

```{r}
col_names = attr(trainingData, ".Dimnames")[[2]]
col_names %>% head()

cat("\nCreating the trees of the xgboost model...")
trees = xgb.model.dt.tree(col_names, model = xgb.model, trees = trees_idx)

trees %>% 
  mutate(Feature = str_trunc(Feature, width=12, side="left")) %>% 
  select(-Missing)
# %>%
#   mutate_at(.vars = vars("Quality","Cover"), .funs = round)

```

「LeafノードのQualityは0じゃないの？」と思うかもしれないが、マニュアルに

> Quality: either the split gain (change in loss) **or the leaf value**

と書いてあり、LeafノードではQualityのセルが**予測結果の格納場所として流用されている**ことに注意。（あとからこれを利用する）((論文を読んでからコードを読み進めていて、ここではまった。))

[`predict(..., predleaf = TRUE)`](https://www.rdocumentation.org/packages/xgboost/versions/0.71.2/topics/predict.xgb.Booster)を指定することで、予測値の代わりに訓練データのインスタンスが各treeで所属するLeafのノード番号を取得できる。
今回は`NROW(trainingData)=5000, nrounds = 3`なので、5000行3列の所属Leafの行列が得られる。

```{r}
cat("\nGetting the leaf nodes for the training set observations...")
nodes.train = predict(xgb.model, trainingData, predleaf = TRUE)

nodes.train %>% dim()
nodes.train <- NULL
```

ただし、取得されたのち、学習に使われたインスタンスの予測結果の情報が、`buildExplainer()`のどこかで使われている形跡は見当たらなかった((この実装、全般的にレガシーが残っている傾向があって、追いかけていったら途中で行き止まりだった、みたいなことが、あちこちである))。

## 予測値の再分配

`xgboostExplainer`は、Leafの予測値を親ノードに再分配することで、予測結果を分解・説明する。

以下では、`buildExplainer()`のエンジン部分である`xgboostExplainer:::getStatsForTrees()`の一連のステップをトレースする。

```{r, eval=FALSE}

# tree_list = xgboostExplainer:::getStatsForTrees(trees, nodes.train, type = type, base_score = base_score)
# function (trees, nodes.train, type = "binary", base_score = 0.5) 
# {


```

なお、関数内部で`data.table::copy()'により、`- attr(*, ".internal.selfref")=<externalptr>`で、**データの更新が参照渡しとして行われている**のに注意。

## Cover (H)の再計算

`xgb.model.dt.tree()`の情報から、各ノードまでの二次の勾配の和を取り出す。これはCover列の情報そのもののはずなのだが、`xgb.model.dt.tree()`の出力するCoverは精度に問題がある((途中の丸め方に問題があるのかな))とのことで、きちんととした精度で計算しなおす。

LeafノードのCoverから、逆向きにたどりながら足し合わせていけばよい

```{r}
type = "binary"
base_score = 0.5

tree_datatable = data.table::copy(trees)

tree_datatable[, leaf := (Feature == "Leaf")]
non.leaves = which(tree_datatable[, leaf] == F)

tree_datatable[, H := Cover]

cat("\n\nRecalculating the cover for each non-leaf... \n")
print.counter = 1
for (i in rev(non.leaves)) {
  left = tree_datatable[i, Yes]
  right = tree_datatable[i, No]
  
  tree_datatable[i, H := (tree_datatable[ID == left, H] + 
                            tree_datatable[ID == right, H])]
  
  if(print.counter < 5){
    print(i - 1)
    
    bind_rows(tree_datatable[i, ],
              tree_datatable[ID == left, ], 
              tree_datatable[ID == right,]) %>% 
      select(-Tree,-Node,-Feature,-Split,-Missing) %>% 
      print()
    print.counter <- print.counter + 1
  }
}

```

CoverとHを比べてみると、たしかに足し算の結果がずれている..。

## 勾配(G)とweightの再計算

`xgboost::xgb.model.dt.tree()`の出力のうち、LeafノードではQualityのセルが**予測結果の格納場所として流用されている**ので、これを起点にして、各親ノードにweightを再分配する。

定義上、$w^*_j=-\frac{G_j}{H_j+\lambda}$ なのだが、これ以降$\lambda$が入っていないので、$G_j = \sum_{i \in I_j}^{} g_i$ とは厳密な意味では違うのかもしれないが、ここは読み取れなかった。

```{r, results="hide"}
base_weight = log(base_score/(1 - base_score))

tree_datatable[, previous_weight := base_weight]
tree_datatable[1, previous_weight:=0]

tree_datatable[leaf==T, weight := base_weight + Quality]

tree_datatable[leaf==T, G := -weight * H]

```
```{r, include=FALSE}
tree_datatable %>% 
  select(-Tree,-Node,-Feature,-Split,-Missing) %>% 
  knitr::kable()
```

以降は、round(Tree)単位に分けて処理を進める。

```{r}
tree_list = split(tree_datatable,as.factor(tree_datatable$Tree))
tree_list %>% str(1)
```

各roundのLeaf以外の親ノードの$G$を計算する。定義から、$I_j = I_{left, j}+I_{right, j}$ なので、$G_j = \sum_{i \in I_{left,j}}^{} g_i + \sum_{i \in I_{right,j}}^{} g_i = G_{L,j} + G_{R,j}$ の足し算を計算すればよい((論文中には、GR ← G − GL$と書いてある（Algrithm.1）))。

```{r}
num_tree_list = length(tree_list)
treenums =  as.character(0:(num_tree_list-1))
t = 0
cat('\n\nFinding the stats for the xgboost trees...\n')
# pb <- txtProgressBar(style=3)
for (tree in tree_list){
  t=t+1
  num_nodes = nrow(tree)
  non_leaf_rows = rev(which(tree[,leaf]==F))
  for (r in non_leaf_rows){
    left = tree[r,Yes]
    right = tree[r,No]
    leftG = tree[ID==left,G]
    rightG = tree[ID==right,G]
    
    tree[r,G:=leftG+rightG]
    w=tree[r,-G/H]
    
    tree[r,weight:=w]
    tree[ID==left,previous_weight:=w]
    tree[ID==right,previous_weight:=w]
    
    # bind_rows(tree[r,], tree[ID==left,], tree[ID==right,]) %>% print()
  }
  
  tree[,uplift_weight:=weight-previous_weight]
  # setTxtProgressBar(pb, t / num_tree_list)
}

tree_list %>% str(1)

```

## Tree Breakdownの構築

`buildExplainerFromTreeList()`を実行すると、

* 各treeで`getTreeBreakdown()`が呼ばれ、
    * 各Leafで` getLeafBreakdown()`が呼ばれ、
        * `findPath()`がLeaf → root node までのパスを取得
        * Leaf → root nodeまでのupliftに基づいて、各パスのImpactを集計
        * root nodeのupliftをInterceptとする

```{r}
cat("\n\nSTEP 2 of 2")
explainer = xgboostExplainer:::buildExplainerFromTreeList(tree_list, col_names)

explainer %>% str(0)
```

本質的な部分ではないので、step-by-stepは省略。`explainer`の中身は以前見た通り。

```{r}

showWaterfall(xgb.model, explainer, xgb.test.data, test.data,  2, type = "binary")

```

