---
title: "XGBoostExplainerが何をやっているか調べる"
author: Satoshi Kato (@katokohaku)
output: 
  html_document:
    keep_md: yes
    toc: yes
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)

knitr::opts_knit$set(
  progress = TRUE, 
  verbose = TRUE, 
  root.dir = "."
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

# 目的

今回は、`xgboostExplainer`によって、xgboostのモデルから**学習したモデルからルールがどうやって抽出されているか**にフォーカスして追いかける。

# 関連シリーズ

1. とりあえず使ってみる
2. 予測結果の可視化プロセスをstep-by-stepで実行する
3. 学習したxgboostのルール抽出をstep-by-stepで実行する（この記事）
4. 予測結果のbreakdownをstep-by-stepで実行する


# `buildExplainer()`の中身を抜き書きしながら眺める

## 準備：XGBモデルの学習と予測

`xgboostExplainer`のマニュアルにあるexampleからコピペ。

```{r train.and.predict, message=FALSE, results="hide"}
require(tidyverse)
library(xgboost)
library(xgboostExplainer)

set.seed(123)

data(agaricus.train, package='xgboost')

X = as.matrix(agaricus.train$data)
y = agaricus.train$label
table(y)
train_idx = 1:5000

train.data = X[train_idx,]
test.data = X[-train_idx,]

xgb.train.data <- xgb.DMatrix(train.data, label = y[train_idx])
xgb.test.data <- xgb.DMatrix(test.data)

param <- list(objective = "binary:logistic")
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=3)

```

# 学習したxgboostのルール抽出をstep-by-stepで実行する

```{r}
# explainer = buildExplainer(xgb.model,xgb.train.data, type="binary", base_score = 0.5, trees = NULL)
# function (xgb.model, trainingData, type = "binary", base_score = 0.5, trees_idx = NULL) 
# {
trainingData = xgb.train.data
type = "binary" 
base_score = 0.5
trees_idx = NULL
col_names = attr(trainingData, ".Dimnames")[[2]]
col_names

cat("\nCreating the trees of the xgboost model...")
trees = xgb.model.dt.tree(col_names, model = xgb.model, trees = trees_idx)

str(trees)

```

```{r}

cat("\nGetting the leaf nodes for the training set observations...")
nodes.train = predict(xgb.model, trainingData, predleaf = TRUE)

```

```{r, eval=FALSE}
# cat("\nBuilding the Explainer...")
# cat("\n\nSTEP 2 of 2")
# tree_list = xgboostExplainer:::getStatsForTrees(trees, nodes.train, type = type, base_score = base_score)
function (trees, nodes.train, type = "binary", base_score = 0.5) 
{
  tree_list = copy(trees)
  tree_list[, `:=`(leaf, Feature == "Leaf")]
  tree_list[, `:=`(H, Cover)]
  non.leaves = which(tree_list[, leaf] == F)
  cat("\n\nRecalculating the cover for each non-leaf... \n")
  pb <- txtProgressBar(style = 3)
  j = 0
  for (i in rev(non.leaves)) {
    left = tree_list[i, Yes]
    right = tree_list[i, No]
    tree_list[i, `:=`(H, tree_list[ID == left, H] + tree_list[ID == right, H])]
    j = j + 1
    setTxtProgressBar(pb, j/length(non.leaves))
  }
  if (type == "regression") {
    base_weight = base_score
  }
  else {
    base_weight = log(base_score/(1 - base_score))
  }
  tree_list[leaf == T, `:=`(weight, base_weight + Quality)]
  tree_list[, `:=`(previous_weight, base_weight)]
  tree_list[1, `:=`(previous_weight, 0)]
  tree_list[leaf == T, `:=`(G, -weight * H)]
  tree_list = split(tree_list, as.factor(tree_list$Tree))
  num_tree_list = length(tree_list)
  treenums = as.character(0:(num_tree_list - 1))
  t = 0
  cat("\n\nFinding the stats for the xgboost trees...\n")
  pb <- txtProgressBar(style = 3)
  for (tree in tree_list) {
    t = t + 1
    num_nodes = nrow(tree)
    non_leaf_rows = rev(which(tree[, leaf] == F))
    for (r in non_leaf_rows) {
      left = tree[r, Yes]
      right = tree[r, No]
      leftG = tree[ID == left, G]
      rightG = tree[ID == right, G]
      tree[r, `:=`(G, leftG + rightG)]
      w = tree[r, -G/H]
      tree[r, `:=`(weight, w)]
      tree[ID == left, `:=`(previous_weight, w)]
      tree[ID == right, `:=`(previous_weight, w)]
    }
    tree[, `:=`(uplift_weight, weight - previous_weight)]
    setTxtProgressBar(pb, t/num_tree_list)
  }
  return(tree_list)
}

```

```{r, eval=FALSE}
# cat("\n\nSTEP 2 of 2")
# explainer = xgboostExplainer:::buildExplainerFromTreeList(tree_list, col_names)
function (tree_list, col_names) 
{
  tree_list_breakdown <- vector("list", length(col_names) + 3)
  names(tree_list_breakdown) = c(col_names, "intercept", "leaf", "tree")
  num_trees = length(tree_list)
  cat("\n\nGetting breakdown for each leaf of each tree...\n")
  pb <- txtProgressBar(style = 3)
  for (x in 1:num_trees) {
    tree = tree_list[[x]]
    tree_breakdown = getTreeBreakdown(tree, col_names)
    tree_breakdown$tree = x - 1
    tree_list_breakdown = rbindlist(append(list(tree_list_breakdown), list(tree_breakdown)))
    setTxtProgressBar(pb, x/num_trees)
  }
  return(tree_list_breakdown)
}

```


```{r, eval=FALSE}
showWaterfall(xgb.model, explainer, xgb.test.data, test.data,  2, type = "binary")

```

次回は、`xgboostExplainer`が抽出したルールをもとにどのような手続きで個別のインスタンスの予測結果を分解しているのか詳細に見ていく。