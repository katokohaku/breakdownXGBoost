---
title: "XGBoostExplainerが何をやっているか調べる"
author: Satoshi Kato (@katokohaku)
output: 
  html_document:
    keep_md: yes
    toc: yes
  md_document:
    variant: markdown_github
---

```{r setup, include=FALSE}
require(tidyverse)
require(magrittr)

knitr::opts_knit$set(
  progress = TRUE, 
  verbose = TRUE, 
  root.dir = "."
)
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  message = FALSE
)
```

# 目的

今回は、`xgboostExplainer`によって、xgboostのモデルと予測結果から**何が取り出され、どう捌かれているか**にフォーカスして追いかける。

# 関連シリーズ

1. [とりあえず使ってみる](http://kato-kohaku-0.hatenablog.com/entry/2018/12/14/002253)
2. 予測結果の可視化プロセスをstep-by-stepで実行する（この記事）
3. 学習したxgboostのルール抽出をstep-by-stepで実行する
4. 予測結果のbreakdownをstep-by-stepで実行する


# 予測結果の可視化プロセスをstep-by-stepで眺める

`showWaterfall(..., type = "binary")`の中身を抜き書きしながら、都度、何が取り出されているか見ていく。

なお、この記事以降では、`objective = "binary:logistic"`を扱って進める。`objective = "reg:linear"`はよりシンプルな手続きであり、前者がわかれば後者は自然に理解できる。

## 準備：XGBモデルの学習と予測

`xgboostExplainer`のマニュアルにあるexampleからコピペ。

```{r train.and.predict, message=FALSE, results="hide"}
require(tidyverse)
library(xgboost)
library(xgboostExplainer)

set.seed(123)

data(agaricus.train, package='xgboost')

X = as.matrix(agaricus.train$data)
y = agaricus.train$label
table(y)
train_idx = 1:5000

train.data = X[train_idx,]
test.data = X[-train_idx,]

xgb.train.data <- xgb.DMatrix(train.data, label = y[train_idx])
xgb.test.data <- xgb.DMatrix(test.data)

param <- list(objective = "binary:logistic")
xgb.model <- xgboost(param =param,  data = xgb.train.data, nrounds=3)

```

## 可視化の手続き(overview)

あるインスタンスの予測結果を可視化する手順は：

1. 学習したxgboostのモデルから予測ルールを抽出する
2. `showWaterfall()`のなかで
    1. 指定したインスタンスの予測結果を予測ルールから分解再構成する
    2. 分解再構成した予測結果をwatarfall chartで可視化する

なお、

* `buildExplainer()`が**どうやって**モデルから予測ルールを抽出しているか
* `explainPredictions()`が**どうやって**分解再構成するか

については今回は触れない。

### 学習したxgboostのモデルから予測ルールを抽出する

`buildExplainer()`が抽出した予測ルール(ノード)を眺める。

```{r, results="hide"}
library(xgboostExplainer)

explainer = buildExplainer(xgb.model,xgb.train.data, type="binary", base_score = 0.5, trees = NULL)

```

今回はルールの数が多いので、一度も登場しないものを除外してみる：

```{r}
nonzero.cols <- abs(explainer) %>% 
  colSums() %>% 
  is_greater_than(0.0) %>% 
  which()

explainer %>% 
  select(nonzero.cols %>% rev()) %>% 
  set_colnames(
    colnames(.) %>% 
      str_trunc(width=10, side="left"))


```

`nrounds = 3`に対応する`tree`が３、それぞれの木のleafにたどり着くためのルールの組み合わせと、更新される予測の量がわかる。


### 指定したインスタンスの予測結果を予測ルールから分解再構成する


指定したインスタンスの予測結果を、`buildExplainer()`が再構成した結果(rules breakdown)を眺める。

```{r, results="hide"}
# showWaterfall(xgb.model, explainer, xgb.test.data, test.data,  2, type = "binary")

DMatrix = xgb.test.data
data.matrix = test.data
idx = 2
type = "binary"
threshold = 1e-04
limits = c(NA, NA)

breakdown = explainPredictions(xgb.model, explainer, slice(DMatrix, as.integer(idx)))

```

ルールが多いので非ゼロのものだけ眺める。
```{r}
breakdown_summary = as.matrix(breakdown)[1, ]
breakdown_summary[which(abs(breakdown_summary) > 0.0)]

```


今回は、`objective = "binary:logistic"`なので、予測値(対数オッズ)は逆ロジット変換により予測結果(クラス所属確率)に変換される
```{r}
(weight = rowSums(breakdown))

(pred = 1/(1 + exp(-weight)))

```

### 分解再構成した予測結果を可視化する

プロットの方針は以下の通り。

1. ベースライン(Intercept)を先頭に表示、残りをインパクト(貢献量)が大きい順に表示
2. インパクトが微小(`< threshold`)のルールは、その他(`other_impact`)にまとめる

```{r}
data_for_label = data.matrix[idx, ]
i = order(abs(breakdown_summary), decreasing = TRUE)
breakdown_summary = breakdown_summary[i]
data_for_label = data_for_label[i]

intercept = breakdown_summary[names(breakdown_summary) == "intercept"]
data_for_label = data_for_label[names(breakdown_summary) != "intercept"]
breakdown_summary = breakdown_summary[names(breakdown_summary) != "intercept"]

```


`threshold`の値はオプション指定で変更でき、インパクトの大きなルールだけに絞り込んで表示できる。やたら細かいルールがいっぱい生えちゃったとかでもない限り((tree depthのチューニングに失敗してoverfitしてそうではあるし、一つのインスタンスの説明用途としては失敗しているともいえる。))`threshold`はデフォルトのままで良さそう。

```{r}
i_other = which(abs(breakdown_summary) < threshold)
other_impact = 0
if (length(i_other > 0)) {
  other_impact = sum(breakdown_summary[i_other])
  names(other_impact) = "other"
  breakdown_summary = breakdown_summary[-i_other]
  data_for_label = data_for_label[-i_other]
}
if (abs(other_impact) > 0) {
  breakdown_summary = c(intercept, breakdown_summary, other_impact)
  data_for_label = c("", data_for_label, "")
  labels = paste0(names(breakdown_summary), " = ", data_for_label)
  labels[1] = "intercept"
  labels[length(labels)] = "other"
} else {
  breakdown_summary = c(intercept, breakdown_summary)
  data_for_label = c("", data_for_label)
  labels = paste0(names(breakdown_summary), " = ", data_for_label)
  labels[1] = "intercept"
}

breakdown_summary %>% t()
```

頑張って並び替えた。

### watarfall chartによる予測結果の描画

算出した値の出力。`getinfo`の箇所はラベルの実名か何かを取ろうとしてる？　キノコのデータだとスキップされる。

```{r}

if (!is.null(getinfo(DMatrix, "label"))) {
  cat("\nActual: ", getinfo(slice(DMatrix, as.integer(idx)), "label"))
}
cat("\nPrediction: ", pred)
cat("\nWeight: ", weight)
cat("\nBreakdown")
cat("\n")
print(breakdown_summary)

```


予測結果(クラス所属確率)で表示するための工夫をする。`inverse_logit_labels`と`logit`は、y軸を対数オッズからクラス所属確率に読み替えるための関数。

((inverse_logit_trans`はコメントアウトしても動作する。レガシーだろうか？))

```{r}
inverse_logit_trans <- scales::trans_new("inverse logit", transform = plogis, inverse = qlogis)

inverse_logit_labels = function(x) {
  return(1/(1 + exp(-x)))
}

logit = function(x) {
  return(log(x/(1 - x)))
}

ybreaks <- logit(seq(2, 98, 2)/100)

waterfalls::waterfall(
  values = breakdown_summary,
  rect_text_labels = round(breakdown_summary, 2),
  labels = labels, 
  total_rect_text = round(weight, 2),
  calc_total = TRUE,
  total_axis_text = "Prediction") + 
  scale_y_continuous(labels = inverse_logit_labels,
                     breaks = ybreaks, 
                     limits = limits) + 
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

`waterfalls::waterfall()`は、外部のパッケージをそのまま利用する。[開発元](https://jameshoward.us/software/waterfall/)を参照されたい。

次回は、`buildExplainer()`が、どのような手続きで指定したインスタンスの予測結果を分解再構成しているのか詳細に見ていく。